{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5e77dd-5a95-4390-af55-67e3b190f405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/sireeshabyreddy96@gmail.com/real-time-weather-pipeline/Medalian_notebooks/Slack_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf806090-9156-4cb0-8e07-b57e2b38b92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, monotonically_increasing_id\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "bronze_table = \"weather_catalog.raw.weather_bronze\"\n",
    "silver_table = \"weather_catalog.processed.valid_readings\"\n",
    "dlq_table = \"weather_catalog.processed.dlq_silver\"\n",
    "log_table = \"weather_catalog.logging.silver_ingestion_logs\"\n",
    "\n",
    "batch_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read Bronze Data\n",
    "# ----------------------------\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "\n",
    "df_bronze.display(5, truncate=False)\n",
    "df_bronze.printSchema()\n",
    "\n",
    "send_slack_message(\n",
    "    f\" Bronze data loaded | Batch: {batch_id} | Rows: {df_bronze.count()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d183b0a-4a4d-4398-ab34-10a9228083a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755883389700}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Step 1: Cast columns\n",
    "df_casted = df_bronze.select(\n",
    "    to_timestamp(col(\"date_time\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"date_time\"),\n",
    "    col(\"maxtempC\").cast(\"double\"),\n",
    "    col(\"mintempC\").cast(\"double\"),\n",
    "    col(\"totalSnow_cm\").cast(\"double\"),\n",
    "    col(\"sunHour\").cast(\"double\"),\n",
    "    col(\"uvIndex\").cast(\"int\"),\n",
    "    col(\"moon_illumination\").cast(\"int\"),\n",
    "    col(\"moonrise\").cast(\"string\"),\n",
    "    col(\"moonset\").cast(\"string\"),\n",
    "    col(\"sunrise\").cast(\"string\"),\n",
    "    col(\"sunset\").cast(\"string\"),\n",
    "    col(\"DewPointC\").cast(\"int\"),\n",
    "    col(\"FeelsLikeC\").cast(\"int\"),\n",
    "    col(\"HeatIndexC\").cast(\"int\"),\n",
    "    col(\"WindChillC\").cast(\"int\"),\n",
    "    col(\"WindGustKmph\").cast(\"int\"),\n",
    "    col(\"cloudcover\").cast(\"int\"),\n",
    "    col(\"humidity\").cast(\"int\"),\n",
    "    col(\"precipMM\").cast(\"double\"),\n",
    "    col(\"pressure\").cast(\"int\"),\n",
    "    col(\"tempC\").cast(\"int\"),\n",
    "    col(\"visibility\").cast(\"int\"),\n",
    "    col(\"winddirDegree\").cast(\"int\"),\n",
    "    col(\"windspeedKmph\").cast(\"int\"),\n",
    "    col(\"City\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Step 2: Schema info\n",
    "df_casted.printSchema()\n",
    "send_slack_message(\"Schema casted successfully for Bronze data.\")\n",
    "\n",
    "# Step 3: Show a sample\n",
    "df_casted.display(5, truncate=False)\n",
    "send_slack_message(f\"Sample data displayed. Row count: {df_casted.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ab02d7-83d0-4fb1-99db-fcd6a6ec6045",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":399},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755882362616}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Step 1: Count nulls per column\n",
    "null_counts = df_casted.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_casted.columns])\n",
    "null_counts.display()\n",
    "\n",
    "# Step 2: Collect null counts as a dictionary\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Step 3: Format message for Slack\n",
    "message = \"Null count per column:\\n\"\n",
    "for col_name, count in null_counts_dict.items():\n",
    "    message += f\"{col_name}: {count}\\n\"\n",
    "\n",
    "# Step 4: Send to Slack\n",
    "send_slack_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3110628-73a7-487e-bcbd-2317d6524d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where more than 50% of columns are null\n",
    "threshold = int(len(df_casted.columns) * 0.5)\n",
    "df_cleaned = df_casted.dropna(thresh=threshold)\n",
    "\n",
    "# Count rows before and after\n",
    "rows_before = df_casted.count()\n",
    "rows_after = df_cleaned.count()\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "# Display sample\n",
    "df_cleaned.display(5, truncate=False)\n",
    "\n",
    "# Send Slack notification\n",
    "message = (f\"Dropped rows with >50% nulls\\n\"\n",
    "           f\"Rows before: {rows_before}\\n\"\n",
    "           f\"Rows after: {rows_after}\\n\"\n",
    "           f\"Rows dropped: {rows_dropped}\")\n",
    "send_slack_message(message)\n",
    "df_cleaned.show()\n",
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb1ebbf-84b7-4170-a69d-89533a9383f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "\n",
    "#  Remove rows with 0 in critical numeric columns\n",
    "critical_cols = [\"maxtempC\", \"mintempC\", \"humidity\", \"pressure\", \"windspeedKmph\"]\n",
    "for c in critical_cols:\n",
    "    df_casted = df_casted.filter(col(c) != 0)\n",
    "send_slack_message(f\" Removed rows with 0 in critical columns: {', '.join(critical_cols)}\")\n",
    "\n",
    "#  Remove rows with empty/UNKNOWN in string columns\n",
    "string_cols = [f.name for f in df_casted.schema.fields if str(f.dataType) == \"StringType\"]\n",
    "for c in string_cols:\n",
    "    df_casted = df_casted.filter((col(c) != \"UNKNOWN\") & (col(c) != \"\"))\n",
    "send_slack_message(f\" Removed rows with empty/UNKNOWN in string columns: {', '.join(string_cols)}\")\n",
    "\n",
    "#  Check remaining nulls\n",
    "null_counts = df_casted.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_casted.columns])\n",
    "display(null_counts)\n",
    "\n",
    "#  Count rows after cleaning\n",
    "rows_after_cleaning = df_casted.count()\n",
    "send_slack_message(f\" Row count after cleaning: {rows_after_cleaning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39faf864-059c-4f8d-a26f-a46d20c859e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Replace NULL values with 0 for numeric columns\n",
    "df_filled = df_casted.fillna(0)\n",
    "\n",
    "# Verify null counts again\n",
    "null_check = df_filled.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_filled.columns])\n",
    "display(null_check)\n",
    "\n",
    "\n",
    "# Send Slack notification\n",
    "message = (f\" Nulls filled with 0 for numeric columns\\n\"\n",
    "           f\"Rows before filling: {rows_before}\\n\"\n",
    "           f\"Rows after filling: {rows_after}\")\n",
    "send_slack_message(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61ffcca-e6fe-4aa0-a8a0-e2c098def7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Count nulls per column, keep original types intact\n",
    "null_counts = df_filled.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_filled.columns])\n",
    "null_counts.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1d03c1-6bed-4796-8d35-3e528b76a410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define columns with potential outliers to remove or clean\n",
    "outlier_cols = [\n",
    "    \"maxtempC\", \"mintempC\", \"totalSnow_cm\", \"sunHour\", \"uvIndex\",\n",
    "    \"DewPointC\", \"FeelsLikeC\", \"HeatIndexC\", \"WindChillC\", \"WindGustKmph\",\n",
    "    \"cloudcover\", \"humidity\", \"precipMM\", \"pressure\", \"tempC\", \"visibility\",\n",
    "    \"winddirDegree\", \"windspeedKmph\"\n",
    "]\n",
    "\n",
    "# Filter out rows with invalid values (-999 or null) in any of these columns\n",
    "for col in outlier_cols:\n",
    "    df_filled = df_filled.filter((F.col(col).isNotNull()) & (F.col(col) != -999))\n",
    "\n",
    "# Count duplicates after removing outliers\n",
    "duplicate_count = df_filled.count() - df_filled.dropDuplicates().count()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Optional: Show top 10 duplicate rows\n",
    "df_filled.groupBy(df_filled.columns).count().filter(\"count > 1\").display(10, truncate=False)\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = df_filled.dropDuplicates()\n",
    "\n",
    "# Send Slack notification\n",
    "if duplicate_count > 0:\n",
    "    send_slack_message(f\"Removed {duplicate_count} duplicate rows.\")\n",
    "else:\n",
    "    send_slack_message(\"No duplicate rows found.\")\n",
    "\n",
    "# df_no_duplicates is now your cleaned DataFrame without duplicates or outliers\n",
    "df_no_duplicates.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687c75ef-823a-4c66-b5b9-98a3bced72f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Function to remove outliers using IQR for all numeric columns\n",
    "def remove_outliers(df, numeric_cols):\n",
    "    for c in numeric_cols:\n",
    "        # Calculate Q1 and Q3\n",
    "        q1, q3 = df.approxQuantile(c, [0.25, 0.75], 0.05)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        # Filter rows within bounds\n",
    "        df = df.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Select numeric columns (exclude string columns like 'City')\n",
    "numeric_cols = [field.name for field in df_no_duplicates.schema.fields if str(field.dataType) in [\"IntegerType\", \"LongType\", \"FloatType\", \"DoubleType\"]]\n",
    "\n",
    "# Row count before outlier removal\n",
    "row_count_before = df_no_duplicates.count()\n",
    "\n",
    "# Remove outliers\n",
    "df_no_outliers = remove_outliers(df_no_duplicates, numeric_cols)\n",
    "\n",
    "# Row count after outlier removal\n",
    "row_count_after = df_no_outliers.count()\n",
    "\n",
    "# Display results\n",
    "df_no_outliers.display()\n",
    "\n",
    "# Slack notification\n",
    "rows_removed = row_count_before - row_count_after\n",
    "if rows_removed > 0:\n",
    "    send_slack_message(\n",
    "        f\" Outlier removal completed. Rows removed: {rows_removed} | Remaining rows: {row_count_after}\", \n",
    "        level=\"WARNING\"\n",
    "    )\n",
    "else:\n",
    "    send_slack_message(\n",
    "        f\" Outlier removal completed. No rows removed. Total rows: {row_count_after}\"\n",
    "       \n",
    "    )\n",
    "\n",
    "print(\"Row count after outlier removal:\", row_count_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555b8f70-9760-4876-904c-60ee7d6628ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 2: Enrich with Metadata\n",
    "# ----------------------------\n",
    "df_enriched = df_no_outliers.withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "                       .withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "                       .withColumn(\"record_id\", monotonically_increasing_id())\n",
    "\n",
    "df_enriched.display(5)\n",
    "df_enriched.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bb734e-6d19-42b9-8196-e644fd361161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "\n",
    "# Remove rows with null city or date_time\n",
    "# ----------------------------\n",
    "df_dlq = df_enriched.filter(\n",
    "    col(\"city\").isNull() | col(\"date_time\").isNull()\n",
    ")\n",
    "\n",
    "dlq_count = df_dlq.count()\n",
    "print(f\"Rows sent to DLQ: {dlq_count}\")\n",
    "\n",
    "# Slack notification\n",
    "if dlq_count > 0:\n",
    "    send_slack_message(\n",
    "        f\"Validation detected {dlq_count} invalid rows sent to DLQ.\",\n",
    "        level=\"WARNING\"\n",
    "    )\n",
    "else:\n",
    "    send_slack_message(\n",
    "        \" Validation completed. No rows sent to DLQ.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2437402-9fe1-4e65-a859-6e247e95e9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the logging schema exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather_catalog.logging\")\n",
    "\n",
    "# Step 4: Write Silver Data\n",
    "df_enriched.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"weather_catalog.raw.silver_table\")\n",
    "\n",
    "# Step 5: Write DLQ Data\n",
    "dlq_count = df_dlq.count()\n",
    "if dlq_count > 0:\n",
    "    df_dlq.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"weather_catalog.logging .dlq_table\")\n",
    "\n",
    "# Step 6: Slack Notification\n",
    "silver_count = df_enriched.count()\n",
    "message = f\" Weather ETL completed.\\n- Silver Records: {silver_count}\\n- DLQ Records: {dlq_count}\"\n",
    "send_slack_message(message)\n",
    "print(message)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

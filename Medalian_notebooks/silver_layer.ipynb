{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b778e84-e793-462b-87aa-1aadff436604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c1a4af-ef81-4fe6-b7b8-87d5fa74ab42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, monotonically_increasing_id\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "bronze_table = \"weather_catalog.raw.weather_bronze\"\n",
    "silver_table = \"weather_catalog.processed.valid_readings\"\n",
    "dlq_table = \"weather_catalog.processed.dlq_silver\"\n",
    "log_table = \"weather_catalog.logging.silver_ingestion_logs\"\n",
    "\n",
    "batch_id = str(uuid.uuid4())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read Bronze Data\n",
    "# ----------------------------\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Enrich with Metadata\n",
    "# ----------------------------\n",
    "df_enriched = df_bronze.withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "                       .withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "                       .withColumn(\"record_id\", monotonically_increasing_id())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Validation / Error Handling\n",
    "# Example: remove rows with null city or date_time\n",
    "# You can add more validations as needed\n",
    "# ----------------------------\n",
    "df_dlq = df_enriched.filter(\n",
    "    col(\"city\").isNull() | col(\"date_time\").isNull()\n",
    ")\n",
    "\n",
    "df_silver = df_enriched.filter(\n",
    "    col(\"city\").isNotNull() & col(\"date_time\").isNotNull()\n",
    ")\n",
    "\n",
    "# Ensure the schema exists before writing tables\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather_catalog.logs\")\n",
    "\n",
    "# Step 4: Write Silver Data\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(silver_table)\n",
    "\n",
    "# Step 5: Write DLQ Data\n",
    "if df_dlq.count() > 0:\n",
    "    df_dlq.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(dlq_table)\n",
    "\n",
    "# Step 6: Logging / Audit\n",
    "row_count_silver = df_silver.count()\n",
    "row_count_dlq = df_dlq.count()\n",
    "status = \"SUCCESS\" if row_count_silver > 0 else \"FAILED\"\n",
    "\n",
    "log_df = spark.createDataFrame(\n",
    "    [(batch_id, bronze_table, silver_table, row_count_silver, row_count_dlq, status, datetime.datetime.now())],\n",
    "    [\"batch_id\", \"source_table\", \"target_table\", \"rows_ingested\", \"rows_failed\", \"status\", \"ingest_time\"]\n",
    ")\n",
    "\n",
    "log_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)\n",
    "\n",
    "print(f\"✅ Batch {batch_id} Completed | Silver: {row_count_silver}, DLQ: {row_count_dlq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf806090-9156-4cb0-8e07-b57e2b38b92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, monotonically_increasing_id\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "bronze_table = \"weather_catalog.raw.weather_bronze\"\n",
    "silver_table = \"weather_catalog.processed.valid_readings\"\n",
    "dlq_table = \"weather_catalog.processed.dlq_silver\"\n",
    "log_table = \"weather_catalog.logging.silver_ingestion_logs\"\n",
    "\n",
    "batch_id = str(uuid.uuid4())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read Bronze Data\n",
    "# ----------------------------\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "print(\"✅ Bronze data loaded\")\n",
    "df_bronze.show(5, truncate=False)\n",
    "df_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ab02d7-83d0-4fb1-99db-fcd6a6ec6045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Count nulls per column\n",
    "null_counts = df_bronze.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_bronze.columns])\n",
    "null_counts.show(truncate=False)\n",
    "\n",
    "# Show rows with any null values\n",
    "df_bronze.filter(\n",
    "    \" OR \".join([f\"{c} IS NULL\" for c in df_bronze.columns])\n",
    ").display(10, truncate=False)\n",
    "\n",
    "# Total rows\n",
    "print(f\"Total rows in Bronze table: {df_bronze.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db0d1f4-ac8f-45ec-b5d8-8c6cf70add06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count duplicates based on all columns\n",
    "duplicate_count = df_bronze.count() - df_bronze.dropDuplicates().count()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Optional: Show duplicate rows\n",
    "df_bronze.groupBy(df_bronze.columns).count().filter(\"count > 1\").display(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555b8f70-9760-4876-904c-60ee7d6628ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 2: Enrich with Metadata\n",
    "# ----------------------------\n",
    "df_enriched = df_bronze.withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "                       .withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "                       .withColumn(\"record_id\", monotonically_increasing_id())\n",
    "\n",
    "df_enriched.display(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bb734e-6d19-42b9-8196-e644fd361161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 3: Validation / Error Handling\n",
    "# Example: remove rows with null city or date_time\n",
    "# You can add more validations as needed\n",
    "# ----------------------------\n",
    "df_dlq = df_enriched.filter(\n",
    "    col(\"city\").isNull() | col(\"date_time\").isNull()\n",
    ")\n",
    "\n",
    "df_silver = df_enriched.filter(\n",
    "    col(\"city\").isNotNull() & col(\"date_time\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\" Valid rows: {df_silver.count()}\")\n",
    "print(f\" Rows sent to DLQ: {df_dlq.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805c42db-e78c-44c9-adb7-979f20ec8090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example cleaning processes\n",
    "# Remove duplicates from Silver\n",
    "df_silver = df_silver.dropDuplicates()\n",
    "\n",
    "# Trim string columns and remove leading/trailing spaces\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "string_cols = [c.name for c in df_silver.schema.fields if c.dataType == \"StringType\" or \"string\" in str(c.dataType).lower()]\n",
    "for c in string_cols:\n",
    "    df_silver = df_silver.withColumn(c, trim(col(c)))\n",
    "\n",
    "# Fill missing numeric values with 0\n",
    "numeric_cols = [c.name for c in df_silver.schema.fields if \"int\" in str(c.dataType).lower() or \"double\" in str(c.dataType).lower()]\n",
    "for c in numeric_cols:\n",
    "    df_silver = df_silver.fillna({c: 0})\n",
    "\n",
    "print(\"✅ Cleaning completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2437402-9fe1-4e65-a859-6e247e95e9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the logging schema exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather_catalog.logging\")\n",
    "\n",
    "# Step 4: Write Silver Data\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(silver_table)\n",
    "\n",
    "# Step 5: Write DLQ Data\n",
    "if df_dlq.count() > 0:\n",
    "    df_dlq.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(dlq_table)\n",
    "\n",
    "print(f\"✅ Data written | Silver: {df_silver.count()}, DLQ: {df_dlq.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

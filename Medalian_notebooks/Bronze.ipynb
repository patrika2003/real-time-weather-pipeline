{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb5137b-5085-4793-a521-21dfc0ce304f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, monotonically_increasing_id,\n",
    "    to_timestamp, to_date, coalesce, struct, to_json, input_file_name\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# Config (catalog tables)\n",
    "# ------------------------------\n",
    "source_table = \"weather_streaming_database.`22`\"             # Glue source table (numeric name must be backticked)\n",
    "bronze_table = \"weather_catalog.raw.weather_readings\"        # UC Bronze\n",
    "dlq_table    = \"weather_catalog.raw.weather_dlq\"             # UC DLQ\n",
    "log_table    = \"weather_catalog.logging.weather_logging\"     # UC Logging\n",
    "\n",
    "batch_id  = str(uuid.uuid4())\n",
    "job_start = current_timestamp()\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Read from Glue table\n",
    "# ------------------------------\n",
    "try:\n",
    "    df_raw = spark.table(source_table)\n",
    "except Exception as e:\n",
    "    # If even reading fails, log and rethrow\n",
    "    err = str(e)\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(batch_id, source_table, bronze_table, 0, 0, \"FAILURE\", err, job_start, current_timestamp())],\n",
    "        [\"batch_id\",\"source\",\"target\",\"row_count_clean\",\"row_count_dlq\",\"status\",\"error_message\",\"start_time\",\"end_time\"]\n",
    "    )\n",
    "    log_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)\n",
    "    raise\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Enrich + derive partition date\n",
    "# - Your Glue schema shows: city, date_time (string), etc.\n",
    "# - Derive a robust 'date' from date_time for partitioning.\n",
    "# ------------------------------\n",
    "df_enriched = (\n",
    "    df_raw\n",
    "    .withColumn(\"ingest_time\", current_timestamp())\n",
    "    .withColumn(\"source_table\", lit(source_table))\n",
    "    .withColumn(\"batch_id\", lit(batch_id))\n",
    "    .withColumn(\"record_id\", monotonically_increasing_id())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    # Try multiple timestamp patterns; fall back to null if unparsable\n",
    "    .withColumn(\n",
    "        \"ts_parsed\",\n",
    "        coalesce(\n",
    "            to_timestamp(col(\"date_time\")),                                    # default Spark parse\n",
    "            to_timestamp(col(\"date_time\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"date_time\"), \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
    "            to_timestamp(col(\"date_time\"), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "            to_timestamp(col(\"date_time\"), \"dd-MM-yyyy HH:mm:ss\"),\n",
    "            to_timestamp(col(\"date_time\"), \"dd-MM-yyyy\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"date\", to_date(col(\"ts_parsed\")))\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Split clean vs DLQ\n",
    "#   Business rules for Bronze:\n",
    "#   - city NOT NULL\n",
    "#   - date_time NOT NULL\n",
    "#   - date (parsed) NOT NULL\n",
    "#   Everything else goes to DLQ with raw payload + reason\n",
    "# ------------------------------\n",
    "invalid_cond = (col(\"city\").isNull() | col(\"date_time\").isNull() | col(\"date\").isNull())\n",
    "\n",
    "df_dlq = (\n",
    "    df_enriched\n",
    "    .filter(invalid_cond)\n",
    "    .withColumn(\"error_reason\", lit(\"Missing or invalid city/date_time/date\"))\n",
    "    .withColumn(\"raw_payload\", to_json(struct([col(c) for c in df_raw.columns])))\n",
    "    .select(\n",
    "        \"record_id\",\"batch_id\",\"ingest_time\",\"source_table\",\"source_file\",\n",
    "        \"error_reason\",\"raw_payload\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_clean = (\n",
    "    df_enriched\n",
    "    .filter(~invalid_cond)\n",
    "    .drop(\"ts_parsed\")  # keep the table tidy\n",
    ")\n",
    "\n",
    "# Cache before counting/writing to avoid recompute\n",
    "df_dlq_cached = df_dlq.cache()\n",
    "df_clean_cached = df_clean.cache()\n",
    "\n",
    "row_count_dlq = df_dlq_cached.count()\n",
    "row_count_clean = df_clean_cached.count()\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Write DLQ (auto-creates table if missing)\n",
    "# ------------------------------\n",
    "if row_count_dlq > 0:\n",
    "    (df_dlq_cached.write.format(\"delta\").mode(\"append\").saveAsTable(dlq_table))\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Write Bronze (schema evolution + partitioning)\n",
    "#   Partition by city + date (both present after validation)\n",
    "# ------------------------------\n",
    "(df_clean_cached.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")      # ✅ schema evolution\n",
    "    .partitionBy(\"city\", \"date\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 6: Logging / Audit\n",
    "# ------------------------------\n",
    "log_df = spark.createDataFrame(\n",
    "    [(batch_id, source_table, bronze_table, row_count_clean, row_count_dlq, \"SUCCESS\", None, job_start, current_timestamp())],\n",
    "    [\"batch_id\",\"source\",\"target\",\"row_count_clean\",\"row_count_dlq\",\"status\",\"error_message\",\"start_time\",\"end_time\"]\n",
    ")\n",
    "log_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table)\n",
    "\n",
    "print(f\"✅ Batch {batch_id} Completed | Clean: {row_count_clean}, DLQ: {row_count_dlq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa3a755-5240-408e-8b2e-4021e147d589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "git checkout -b feature/add-notebooks\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
